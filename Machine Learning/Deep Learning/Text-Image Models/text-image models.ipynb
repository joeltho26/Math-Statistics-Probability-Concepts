{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958f9fad",
   "metadata": {},
   "source": [
    "1. [Text to Image Generation](#text-to-image-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb614e63",
   "metadata": {},
   "source": [
    "#### Text to Image Generation:\n",
    "- here we use multiple deep neural network architecture\n",
    "    - combination of transformers + diffusion model (UNet)\n",
    "    - earlier, we used GANs but they are often unstable to minor changes\n",
    "\n",
    "- Working:\n",
    "    - Text Processing:\n",
    "        - Tokenization:\n",
    "            - we break them into smaller units \n",
    "        - Text Encoding:\n",
    "            - we perform encoding via T5 transformer model and create embeddings\n",
    "        - we multiply the feature embeddings with positional embeddings to create sequence representation\n",
    "            - position embeddings focuses on the order of the words in the text sequence \n",
    "    - Image generation:\n",
    "        - Forward Diffusion model:\n",
    "            - add noise and convert the image into pure noise\n",
    "        - Image generation in latent space:\n",
    "        - Denoising:\n",
    "            - text encoded and noise image as input and iteratively denoises the image via the text encoding information\n",
    "        - UNet Architecture:\n",
    "        - Upscaling \n",
    "            - using super resolution models\n",
    "    - Training:\n",
    "        - Loss function\n",
    "        - trained on massive dataset\n",
    "        - backbone architecture uses CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414c184",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
