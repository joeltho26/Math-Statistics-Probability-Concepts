{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de358591",
   "metadata": {},
   "source": [
    "1. [Vanishing Gradient](#vanishing-gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e32857",
   "metadata": {},
   "source": [
    "#### Vanishing Gradient:\n",
    "- it occurs in neural network when the derivative value of the output values and the weight become negligible and the weight updation happens in a very small fraction\n",
    "- the weights reaching the global minima values is very very slow or sometimes will not even reach global minima because the weight updation will result in the same weight which is already present\n",
    "- this could be due to the role of activation functions used such as sigmoid or tanh (hyperbolic) functions, improper weight initialization, RNNs (because there are several time steps considering the number of words)\n",
    "- the vanishing gradients can be overcome by using non saturated activation functions, LSTM, better weight initialization, batch normalization, RESNET using skip connection (allows gradients to be bypassed one or more layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb9f31d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
