{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9789ce",
   "metadata": {},
   "source": [
    "1. [Exploding Gradient](#exploding-gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a55ebb",
   "metadata": {},
   "source": [
    "#### Exploding Gradient:\n",
    "- this occurs in neural network when the updation of weights happen by a larger value\n",
    "- while performign backpropogation, the derivative of the output/weights result in a larger weight updation values and creates instability\n",
    "- the gradients which become increasing during each weight updation step\n",
    "- exploding gradient occurs as a result of improper weight initialization, high learning rate, RNNs with long sequences\n",
    "- to solve exploding gradient, we use gradient clipping (scaling the gradients when it exceeds a certain threshold), batch normalization, proper weight initialization, smaller learning rate, LSTM, GRUs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
