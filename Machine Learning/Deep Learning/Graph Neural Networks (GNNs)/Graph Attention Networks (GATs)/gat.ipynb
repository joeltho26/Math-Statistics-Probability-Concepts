{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216c0193",
   "metadata": {},
   "source": [
    "1. [Graph Attention Network](#graph-attention-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e4896",
   "metadata": {},
   "source": [
    "#### Graph Attention Network:\n",
    "- linear transformation of features\n",
    "    - each node's initial feature representation undergoes linear transformation using learnable weights\n",
    "- calculating raw attention scores\n",
    "    - every pair of connected nodes calculate a unnormalized attention score \n",
    "    - concat two transformed features of node and apply learnable attention mechanism (single feed forward neural network and leaky relu activation function)\n",
    "- normalizing attention coefficient\n",
    "    - normalize the raw attention score using softmax function\n",
    "    - ensure the attention coefficient value of all neighbours for a given node is equal to 1\n",
    "- aggregating features with attention weights\n",
    "    - now we generate a new feature represntation by taking the weighted sum of neighbours transformed features along with the attention coefficients\n",
    "- multi head attention\n",
    "    - applies multiple attention mechanism \n",
    "    - the output from these mechanism are either averaged or concatenated \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081292d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
