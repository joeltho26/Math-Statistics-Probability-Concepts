{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Cost Complexity Pruning](#cost-complexity-pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Complexity Pruning:\n",
    "- it is a post pruning process of decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process of Cost Complexity Pruning - Classification:\n",
    "- At first, build the complete decision tree via Gini Index having the most common class label as a leaf node using both training and testing data\n",
    "- Cost complexity pruning which is denoted by a tree score to remove weakest links\n",
    "    - Calculate the tree score to evaluate the performance of the tree for pruning\n",
    "        - Gini Impurity + αT\n",
    "            v- Gini Impurity = Gini Index value\n",
    "            - α = Hyper parameter (find optimal value via hyper parameter tuning and cross validation)\n",
    "            - T = number of Terminal or Leaf nodes\n",
    "\n",
    "Note:\n",
    "- Gini Index value increase as the number of leaf nodes decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We start with α = 0, and calculate the RSS for the fully developed tree i.e. Gini Index = 0.343 (sum of Gini Index value for each leaf node)\n",
    "    - We calculate the tree score\n",
    "- We continue to increase the α value and also prune the most bottom leaf at a point until we record the value for α which gives us a lower tree score than the previous α value\n",
    "- Again, we continue to increase the α value and also prune the most bottom leaf at a point until we record the value for α which gives us a lower tree score than the previous α value\n",
    "- We repeat the process of pruning until there is only one node in the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we again split the data into train and test\n",
    "- We use the train data to build the trees from full sized to various pruned tree using the previously calculated α values\n",
    "- now we apply the testing data to the full sized tree to various pruned trees and find the tree with the lowest Gini Index Value\n",
    "- however to validate, we perform cross validatiion and calculate the Gini Index for all the types of trees built to find the optimum value of α i.e. α = 10000 and its tree"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
